{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import gzip\n",
    "import datetime\n",
    "import requests\n",
    "import csv\n",
    "import mxnet as mx\n",
    "import numpy as np\n",
    "# import contractions\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import d2l\n",
    "# from textblob import TextBlob\n",
    "from nltk.corpus import stopwords\n",
    "from mxnet import init\n",
    "from mxnet import io\n",
    "from mxnet.gluon import loss as gloss\n",
    "from mxnet import gluon\n",
    "from pandas.io.json import json_normalize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data stored from various categories. Categories include ("
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method to load data taken from : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse(path):\n",
    "  g = gzip.open(path, 'rb')\n",
    "  for l in g:\n",
    "    yield eval(l)\n",
    "\n",
    "def getDF(path):\n",
    "  i = 0\n",
    "  df = {}\n",
    "  for d in parse(path):\n",
    "    df[i] = d\n",
    "    i += 1\n",
    "  return pd.DataFrame.from_dict(df, orient='index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Useful Code Snipits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Group by asin which is tthe product ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "MXNetError",
     "evalue": "[14:24:47] src/io/iter_csv.cc:152: Check failed: row.length == shape.Size() (14 vs. 10) The data size in CSV do not match size of shape: specified shape=[10], the csv row-length=14\n\nStack trace returned 10 entries:\n[bt] (0) 0   libmxnet.so                         0x0000001a185cfb90 libmxnet.so + 15248\n[bt] (1) 1   libmxnet.so                         0x0000001a185cf93f libmxnet.so + 14655\n[bt] (2) 2   libmxnet.so                         0x0000001a19b52b05 MXNDListFree + 750005\n[bt] (3) 3   libmxnet.so                         0x0000001a19b521c0 MXNDListFree + 747632\n[bt] (4) 4   libmxnet.so                         0x0000001a19b52e8d MXNDListFree + 750909\n[bt] (5) 5   libmxnet.so                         0x0000001a19b591ec MXNDListFree + 776348\n[bt] (6) 6   libmxnet.so                         0x0000001a19b58cca MXNDListFree + 775034\n[bt] (7) 7   libmxnet.so                         0x0000001a19b5884e MXNDListFree + 773886\n[bt] (8) 8   libsystem_pthread.dylib             0x00007fff6bf8a2eb _pthread_body + 126\n[bt] (9) 9   libsystem_pthread.dylib             0x00007fff6bf8d249 _pthread_start + 66\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMXNetError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-ec3ae58a368d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# print(\"test1\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdata_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCSVIter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_csv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'category_data/filtered_joined_dataset_p1.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m# data_iter = mx.io.CSVIter(data_csv='category_data/filtered_joined_dataset_p1.csv', data_shape=(3,), batch_size=30)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# df = pd.read_csv('category_data/filtered_joined_dataset_p1.csv')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/mxnet/io.py\u001b[0m in \u001b[0;36mcreator\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    939\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'%s can only accept keyword arguments'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0miter_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    940\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 941\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mMXDataIter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    942\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    943\u001b[0m     \u001b[0mcreator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/mxnet/io.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, handle, data_name, label_name, **_)\u001b[0m\n\u001b[1;32m    798\u001b[0m         \u001b[0;31m# load the first batch to get shape information\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    799\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfirst_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 800\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfirst_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    801\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfirst_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    802\u001b[0m         \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfirst_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/mxnet/io.py\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    832\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_debug_at_begin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    833\u001b[0m         \u001b[0mnext_res\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_int\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 834\u001b[0;31m         \u001b[0mcheck_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_LIB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMXDataIterNext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbyref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_res\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    835\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnext_res\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    836\u001b[0m             return DataBatch(data=[self.getdata()], label=[self.getlabel()], pad=self.getpad(),\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/mxnet/base.py\u001b[0m in \u001b[0;36mcheck_call\u001b[0;34m(ret)\u001b[0m\n\u001b[1;32m    249\u001b[0m     \"\"\"\n\u001b[1;32m    250\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMXNetError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpy_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_LIB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMXGetLastError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMXNetError\u001b[0m: [14:24:47] src/io/iter_csv.cc:152: Check failed: row.length == shape.Size() (14 vs. 10) The data size in CSV do not match size of shape: specified shape=[10], the csv row-length=14\n\nStack trace returned 10 entries:\n[bt] (0) 0   libmxnet.so                         0x0000001a185cfb90 libmxnet.so + 15248\n[bt] (1) 1   libmxnet.so                         0x0000001a185cf93f libmxnet.so + 14655\n[bt] (2) 2   libmxnet.so                         0x0000001a19b52b05 MXNDListFree + 750005\n[bt] (3) 3   libmxnet.so                         0x0000001a19b521c0 MXNDListFree + 747632\n[bt] (4) 4   libmxnet.so                         0x0000001a19b52e8d MXNDListFree + 750909\n[bt] (5) 5   libmxnet.so                         0x0000001a19b591ec MXNDListFree + 776348\n[bt] (6) 6   libmxnet.so                         0x0000001a19b58cca MXNDListFree + 775034\n[bt] (7) 7   libmxnet.so                         0x0000001a19b5884e MXNDListFree + 773886\n[bt] (8) 8   libsystem_pthread.dylib             0x00007fff6bf8a2eb _pthread_body + 126\n[bt] (9) 9   libsystem_pthread.dylib             0x00007fff6bf8d249 _pthread_start + 66\n\n"
     ]
    }
   ],
   "source": [
    "# print(\"test1\")\n",
    "df = pd.read_csv('category_data/filtered_joined_dataset_p1.csv')\n",
    "train_iter = mx.io.CSVIter(data_csv = 'category_data/', data_shape = (10,),batch_size = 2)\n",
    "# data_iter = mx.io.CSVIter(data_csv='category_data/filtered_joined_dataset_p1.csv', data_shape=(3,), batch_size=30)\n",
    "\n",
    "\n",
    "# print(df.shape)\n",
    "\n",
    "# temp_iter = mx.io.NDArrayIter(df.values,label,batch_size=25)\n",
    "# print(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature : The star rating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The review-assigned product star rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature : Subjectivity "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The probability of a review and its sentences being subjective. https://textblob.readthedocs.io/en/dev/ and https://planspace.org/20150607-textblob_sentiment/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentiment(polarity=-0.3076923076923077, subjectivity=0.5769230769230769)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TextBlob(\"not a very great calculation\").sentiment\n",
    "## Sentiment(polarity=-0.3076923076923077, subjectivity=0.5769230769230769)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature : TF-IDF  |  Product features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attempt to identify the presence of important topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_group' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-1d81c9661f99>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mget_aspects\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_group\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"120401325X\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"reviewText\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'df_group' is not defined"
     ]
    }
   ],
   "source": [
    "def tf_idf(document):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    X = vectorizer.fit_transform(document)\n",
    "    return X.toarray()\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# A Tokenizer that tokenize a sequence of alphabetical character and drop everything else\n",
    "tokenizer = RegexpTokenizer(r'\\w+') \n",
    "\n",
    "def get_aspects(x):\n",
    "    import contractions\n",
    "    doc = contractions.fix(x)\n",
    "    doc=tokenizer.tokenize(doc) ## Tokenize and extract grammatical components\n",
    "    doc=list(map(lambda i: i.lower(),doc)) ## Normalize text to lower case\n",
    "    doc=[i for i in doc if i not in stop_words] ## Remove common words and retain words not in stop words\n",
    "    doc = [lemmatizer.lemmatize(j) for j in doc]\n",
    "    doc=pd.Series(doc)\n",
    "    doc=doc.value_counts().head(10).index.tolist() ## Get 10 most frequent words\n",
    "    return doc\n",
    "\n",
    "get_aspects(df_group.get_group(\"120401325X\")[\"reviewText\"][5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature : Sensitivity Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking for use of pos/neg words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_group' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-14ab78b2a3b3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0msid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSentimentIntensityAnalyzer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtest_subset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_group\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"120401325X\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"reviewText\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mpos_word_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mneu_word_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_group' is not defined"
     ]
    }
   ],
   "source": [
    "test_subset = tokenizer.tokenize(df_group.get_group(\"120401325X\")[\"reviewText\"][3])\n",
    "pos_word_list=[]\n",
    "neu_word_list=[]\n",
    "neg_word_list=[]\n",
    "\n",
    "for word in test_subset:\n",
    "    if (sid.polarity_scores(word)['compound']) >= 0.5:\n",
    "        pos_word_list.append(word)\n",
    "    elif (sid.polarity_scores(word)['compound']) <= -0.5:\n",
    "        neg_word_list.append(word)\n",
    "    else:\n",
    "        neu_word_list.append(word)                \n",
    "\n",
    "print('Positive :',pos_word_list)        \n",
    "print('Neutral :',neu_word_list)    \n",
    "print('Negative :',neg_word_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature : Readability "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Measures how easy a text is to read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readabilityMetrics(text):\n",
    "    response = requests.post(\"https://ipeirotis-readability-metrics.p.rapidapi.com/getReadabilityMetrics\",\n",
    "      headers={\n",
    "        \"X-RapidAPI-Host\": \"ipeirotis-readability-metrics.p.rapidapi.com\",\n",
    "        \"X-RapidAPI-Key\": \"818e0f0b58mshe4a4d5842f257bep1cc5a6jsne73e02710bab\",\n",
    "        \"Content-Type\": \"application/x-www-form-urlencoded\"\n",
    "      },\n",
    "      params={\n",
    "        \"text\":text                        \n",
    "      }\n",
    "    )\n",
    "    rowEntry = response.json()\n",
    "    rowEntry[\"reviewText\"] = text\n",
    "    return json_normalize(rowEntry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature : Spelling Errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numberOfSpellingErrors(text):\n",
    "    response = requests.get(\"https://montanaflynn-spellcheck.p.rapidapi.com/check/\",\n",
    "      headers={\n",
    "            \"X-RapidAPI-Host\": \"montanaflynn-spellcheck.p.rapidapi.com\",\n",
    "            \"X-RapidAPI-Key\": \"818e0f0b58mshe4a4d5842f257bep1cc5a6jsne73e02710bab\"\n",
    "      },\n",
    "      params={\n",
    "        \"text\":text                        \n",
    "      }\n",
    "    )\n",
    "    return len(response.json()['corrections'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are predicting a value between [0.0, 5.0] we will be using a linear regression model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***CITE*** Method from D2l "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.initialize(init.Normal(sigma=0.01))\n",
    "\n",
    "loss = gloss.L2Loss()\n",
    "def get_net():\n",
    "    net = nn.Sequential()\n",
    "    net.add(nn.Dense(1))\n",
    "    net.initialize()\n",
    "    return net\n",
    "\n",
    "\n",
    "def train(net, train_features, train_labels, test_features, test_labels, num_epochs, learning_rate, weight_decay, batch_size):\n",
    "    train_ls, test_ls = [], []\n",
    "    train_iter = gdata.DataLoader(gdata.ArrayDataset(\n",
    "    train_features, train_labels), batch_size, shuffle=True) # The Adam optimization algorithm is used here.\n",
    "    trainer = gluon.Trainer(net.collect_params(), 'adam', {'learning_rate': learning_rate, 'wd': weight_decay}) \n",
    "    for epoch in range(num_epochs):\n",
    "        for X, y in train_iter: \n",
    "            with autograd.record(): \n",
    "                l = loss(net(X), y)\n",
    "            l.backward()\n",
    "            trainer.step(batch_size)\n",
    "        train_ls.append(log_rmse(net, train_features, train_labels)) \n",
    "        if test_labels is not None:\n",
    "            test_ls.append(log_rmse(net, test_features, test_labels)) \n",
    "    return train_ls, test_ls\n",
    "\n",
    "\n",
    "\n",
    "## Optimization Algorithm\n",
    "trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': 0.03})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classify the Testing Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measure Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions at Check-In"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. How can we make our project more novel? \n",
    "2.\n",
    "3.\n",
    "4.\n",
    "5.\n",
    "6."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
